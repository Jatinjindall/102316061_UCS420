{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm1bBlSKbtH2"
      },
      "source": [
        "# Lab Assignment-9\n",
        "# Cognitive Computing (UCS420)\n",
        "#Name:- Jatin Jindal\n",
        "#Roll Number:- 102316061"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB0kgyvIbtH9"
      },
      "source": [
        "Question 1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
        "technology, food, books, etc.).\n",
        "1. Convert text to lowercase and remove punctuaƟon.\n",
        "2. Tokenize the text into words and sentences.\n",
        "3. Remove stopwords (using NLTK's stopwords list).\n",
        "4. Display word frequency distribuƟon (excluding stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GjYZEvcbtH_",
        "outputId": "da64f522-11f5-43cf-b050-7bdfb1b263fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized Sentences:\n",
            "1: \n",
            "technology is my favorite topic because it constantly evolves and shapes our world\n",
            "i enjoy learning about new gadgets innovative software and breakthroughs in artificial intelligence\n",
            "its fascinating to see how technology impacts industries like healthcare education and entertainment\n",
            "reading tech blogs and watching product launches excites me\n",
            "technology makes our lives more convenient and opens up endless possibilities for the future\n",
            "\n",
            "Filtered Words (After Stopword Removal):\n",
            "['technology', 'favorite', 'topic', 'constantly', 'evolves', 'shapes', 'world', 'enjoy', 'learning', 'new', 'gadgets', 'innovative', 'software', 'breakthroughs', 'artificial', 'intelligence', 'fascinating', 'see', 'technology', 'impacts', 'industries', 'like', 'healthcare', 'education', 'entertainment', 'reading', 'tech', 'blogs', 'watching', 'product', 'launches', 'excites', 'technology', 'makes', 'lives', 'convenient', 'opens', 'endless', 'possibilities', 'future']\n",
            "\n",
            "Word Frequency Distribution:\n",
            "technology: 3\n",
            "favorite: 1\n",
            "topic: 1\n",
            "constantly: 1\n",
            "evolves: 1\n",
            "shapes: 1\n",
            "world: 1\n",
            "enjoy: 1\n",
            "learning: 1\n",
            "new: 1\n",
            "gadgets: 1\n",
            "innovative: 1\n",
            "software: 1\n",
            "breakthroughs: 1\n",
            "artificial: 1\n",
            "intelligence: 1\n",
            "fascinating: 1\n",
            "see: 1\n",
            "impacts: 1\n",
            "industries: 1\n",
            "like: 1\n",
            "healthcare: 1\n",
            "education: 1\n",
            "entertainment: 1\n",
            "reading: 1\n",
            "tech: 1\n",
            "blogs: 1\n",
            "watching: 1\n",
            "product: 1\n",
            "launches: 1\n",
            "excites: 1\n",
            "makes: 1\n",
            "lives: 1\n",
            "convenient: 1\n",
            "opens: 1\n",
            "endless: 1\n",
            "possibilities: 1\n",
            "future: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Install and download necessary packages\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Download the required packages\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download punkt_tab for tab-separated data\n",
        "\n",
        "# 1. Write a unique paragraph\n",
        "paragraph = \"\"\"\n",
        "Technology is my favorite topic because it constantly evolves and shapes our world.\n",
        "I enjoy learning about new gadgets, innovative software, and breakthroughs in artificial intelligence.\n",
        "It's fascinating to see how technology impacts industries like healthcare, education, and entertainment.\n",
        "Reading tech blogs and watching product launches excites me.\n",
        "Technology makes our lives more convenient and opens up endless possibilities for the future.\n",
        "\"\"\"\n",
        "\n",
        "# 2. Convert text to lowercase and remove punctuation\n",
        "paragraph_lower = paragraph.lower()\n",
        "paragraph_clean = paragraph_lower.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# 3. Tokenize into words and sentences\n",
        "# Make sure 'punkt' is downloaded before this\n",
        "words = nltk.word_tokenize(paragraph_clean)\n",
        "sentences = nltk.sent_tokenize(paragraph_clean)\n",
        "\n",
        "# 4. Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "# 5. Display word frequency distribution\n",
        "word_freq = Counter(filtered_words)\n",
        "\n",
        "# Output results\n",
        "print(\"\\nTokenized Sentences:\")\n",
        "for i, sent in enumerate(sentences, 1):\n",
        "    print(f\"{i}: {sent}\")\n",
        "\n",
        "print(\"\\nFiltered Words (After Stopword Removal):\")\n",
        "print(filtered_words)\n",
        "\n",
        "print(\"\\nWord Frequency Distribution:\")\n",
        "for word, freq in word_freq.items():\n",
        "    print(f\"{word}: {freq}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tusWb3EobtIE"
      },
      "source": [
        "Question 2: Stemming and LemmaƟzaƟon\n",
        "1. Take the tokenized words from QuesƟon 1 (aŌer stopword removal).\n",
        "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "3. Apply lemmaƟzaƟon using NLTK's WordNetLemmaƟzer.\n",
        "4. Compare and display results of both techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDEq0swZbtIE",
        "outputId": "7c52de74-ba4f-499c-c610-e4d2521efb4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Porter Stemmed Words: ['technolog', 'favorit', 'topic', 'constantli', 'evolv', 'shape', 'world', 'enjoy', 'learn', 'new', 'gadget', 'innov', 'softwar', 'breakthrough', 'artifici', 'intellig', 'fascin', 'see', 'technolog', 'impact', 'industri', 'like', 'healthcar', 'educ', 'entertain', 'read', 'tech', 'blog', 'watch', 'product', 'launch', 'excit', 'technolog', 'make', 'live', 'conveni', 'open', 'endless', 'possibl', 'futur']\n",
            "\n",
            "Lancaster Stemmed Words: ['technolog', 'favorit', 'top', 'const', 'evolv', 'shap', 'world', 'enjoy', 'learn', 'new', 'gadget', 'innov', 'softw', 'breakthrough', 'art', 'intellig', 'fascin', 'see', 'technolog', 'impact', 'industry', 'lik', 'healthc', 'educ', 'entertain', 'read', 'tech', 'blog', 'watch', 'produc', 'launch', 'excit', 'technolog', 'mak', 'liv', 'conveny', 'op', 'endless', 'poss', 'fut']\n",
            "\n",
            "Lemmatized Words: ['technology', 'favorite', 'topic', 'constantly', 'evolves', 'shape', 'world', 'enjoy', 'learning', 'new', 'gadget', 'innovative', 'software', 'breakthrough', 'artificial', 'intelligence', 'fascinating', 'see', 'technology', 'impact', 'industry', 'like', 'healthcare', 'education', 'entertainment', 'reading', 'tech', 'blog', 'watching', 'product', 'launch', 'excites', 'technology', 'make', 'life', 'convenient', 'open', 'endless', 'possibility', 'future']\n",
            "\n",
            "Comparison:\n",
            "Original: technology, Porter: technolog, Lancaster: technolog, Lemma: technology\n",
            "Original: favorite, Porter: favorit, Lancaster: favorit, Lemma: favorite\n",
            "Original: topic, Porter: topic, Lancaster: top, Lemma: topic\n",
            "Original: constantly, Porter: constantli, Lancaster: const, Lemma: constantly\n",
            "Original: evolves, Porter: evolv, Lancaster: evolv, Lemma: evolves\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK resource (if not already downloaded)\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# --- Q2: Stemming and Lemmatization ---\n",
        "\n",
        "# Initialize stemmers and lemmatizer\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Stemming\n",
        "porter_stemmed_words = [porter_stemmer.stem(word) for word in filtered_words]\n",
        "lancaster_stemmed_words = [lancaster_stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "# Output\n",
        "print(\"\\nPorter Stemmed Words:\", porter_stemmed_words)\n",
        "print(\"\\nLancaster Stemmed Words:\", lancaster_stemmed_words)\n",
        "print(\"\\nLemmatized Words:\", lemmatized_words)\n",
        "\n",
        "# Comparison (You can add more detailed analysis here)\n",
        "print(\"\\nComparison:\")\n",
        "for i in range(min(len(filtered_words), 5)):  # Compare first 5 words\n",
        "    print(f\"Original: {filtered_words[i]}, Porter: {porter_stemmed_words[i]}, Lancaster: {lancaster_stemmed_words[i]}, Lemma: {lemmatized_words[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYSEq_UTbtIG"
      },
      "source": [
        "Question 3. Regular Expressions and Text Spliƫng\n",
        "1. Take their original text from QuesƟon 1.\n",
        "2. Use regular expressions to:\n",
        "a. Extract all words with more than 5 leƩers.\n",
        "b. Extract all numbers (if any exist in their text).\n",
        "c. Extract all capitalized words.\n",
        "3. Use text spliƫng techniques to:\n",
        "a. Split the text into words containing only alphabets (removing digits and special\n",
        "characters).\n",
        "b. Extract words starƟng with a vowel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKy-nyzPbtIG",
        "outputId": "4614c6ca-5389-47b8-8873-4f6212953522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Words with more than 5 letters: ['Technology', 'favorite', 'because', 'constantly', 'evolves', 'shapes', 'learning', 'gadgets', 'innovative', 'software', 'breakthroughs', 'artificial', 'intelligence', 'fascinating', 'technology', 'impacts', 'industries', 'healthcare', 'education', 'entertainment', 'Reading', 'watching', 'product', 'launches', 'excites', 'Technology', 'convenient', 'endless', 'possibilities', 'future']\n",
            "\n",
            "Numbers: []\n",
            "\n",
            "Capitalized words: ['Technology', 'I', 'It', 'Reading', 'Technology']\n",
            "\n",
            "Words with only alphabets: ['Technology', 'is', 'my', 'favorite', 'topic', 'because', 'it', 'constantly', 'evolves', 'and', 'shapes', 'our', 'world', 'I', 'enjoy', 'learning', 'about', 'new', 'gadgets', 'innovative', 'software', 'and', 'breakthroughs', 'in', 'artificial', 'intelligence', 'It', 's', 'fascinating', 'to', 'see', 'how', 'technology', 'impacts', 'industries', 'like', 'healthcare', 'education', 'and', 'entertainment', 'Reading', 'tech', 'blogs', 'and', 'watching', 'product', 'launches', 'excites', 'me', 'Technology', 'makes', 'our', 'lives', 'more', 'convenient', 'and', 'opens', 'up', 'endless', 'possibilities', 'for', 'the', 'future']\n",
            "\n",
            "Words starting with a vowel: ['is', 'it', 'evolves', 'and', 'our', 'I', 'enjoy', 'about', 'innovative', 'and', 'in', 'artificial', 'intelligence', 'It', 'impacts', 'industries', 'education', 'and', 'entertainment', 'and', 'excites', 'our', 'and', 'opens', 'up', 'endless']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Your original paragraph from Question 1\n",
        "paragraph = \"\"\"\n",
        "Technology is my favorite topic because it constantly evolves and shapes our world.\n",
        "I enjoy learning about new gadgets, innovative software, and breakthroughs in artificial intelligence.\n",
        "It's fascinating to see how technology impacts industries like healthcare, education, and entertainment.\n",
        "Reading tech blogs and watching product launches excites me.\n",
        "Technology makes our lives more convenient and opens up endless possibilities for the future.\n",
        "\"\"\"\n",
        "\n",
        "# --- Regular Expressions ---\n",
        "\n",
        "# a. Words with more than 5 letters\n",
        "words_more_than_5 = re.findall(r'\\b\\w{6,}\\b', paragraph)\n",
        "print(\"\\nWords with more than 5 letters:\", words_more_than_5)\n",
        "\n",
        "# b. Numbers (if any)\n",
        "numbers = re.findall(r'\\d+', paragraph)\n",
        "print(\"\\nNumbers:\", numbers)\n",
        "\n",
        "# c. Capitalized words\n",
        "capitalized_words = re.findall(r'\\b[A-Z]\\w*\\b', paragraph)\n",
        "print(\"\\nCapitalized words:\", capitalized_words)\n",
        "\n",
        "# --- Text Splitting ---\n",
        "\n",
        "# a. Words with only alphabets\n",
        "words_only_alphabets = re.findall(r'\\b[a-zA-Z]+\\b', paragraph)\n",
        "print(\"\\nWords with only alphabets:\", words_only_alphabets)\n",
        "\n",
        "# b. Words starting with a vowel\n",
        "words_starting_with_vowel = re.findall(r'\\b[aeiouAEIOU]\\w*\\b', paragraph)\n",
        "print(\"\\nWords starting with a vowel:\", words_starting_with_vowel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoGAHsSWbtIH"
      },
      "source": [
        "Question  4. Custom TokenizaƟon & Regex-based Text Cleaning\n",
        "1. Take original text from QuesƟon 1.\n",
        "2. Write a custom tokenizaƟon funcƟon that:\n",
        "a. Removes punctuaƟon and special symbols, but keeps contracƟons (e.g.,\n",
        "\"isn't\" should not be split into \"is\" and \"n't\").\n",
        "b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains\n",
        "a single token).\n",
        "c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\"\n",
        "should remain as is).\n",
        "3. Use Regex SubsƟtuƟons (re.sub) to:\n",
        "a. Replace email addresses with '<EMAIL>' placeholder.\n",
        "b. Replace URLs with '<URL>' placeholder.\n",
        "c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n",
        "'<PHONE>' placeholder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4e0n_9HbtIH",
        "outputId": "566635a7-4522-44e9-8700-0ceede0b45e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom Tokenized Text: ['Technology', 'is', 'my', 'favorite', 'topic', 'because', 'it', 'constantly', 'evolves', 'and', 'shapes', 'our', 'world', 'I', 'enjoy', 'learning', 'about', 'new', 'gadgets', 'innovative', 'software', 'and', 'breakthroughs', 'in', 'artificial', 'intelligence', 'It', \"'s\", 'fascinating', 'to', 'see', 'how', 'technology', 'impacts', 'industries', 'like', 'healthcare', 'education', 'and', 'entertainment', 'Reading', 'tech', 'blogs', 'and', 'watching', 'product', 'launches', 'excites', 'me', 'Technology', 'makes', 'our', 'lives', 'more', 'convenient', 'and', 'opens', 'up', 'endless', 'possibilities', 'for', 'the', 'future']\n",
            "\n",
            "Text with Emails, URLs, and Phone Numbers Replaced:\n",
            "\n",
            "Technology is my favorite topic because it constantly evolves and shapes our world. \n",
            "I enjoy learning about new gadgets, innovative software, and breakthroughs in artificial intelligence. \n",
            "It's fascinating to see how technology impacts industries like healthcare, education, and entertainment. \n",
            "Reading tech blogs and watching product launches excites me. \n",
            "Technology makes our lives more convenient and opens up endless possibilities for the future.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Your original paragraph from Question 1\n",
        "paragraph = \"\"\"\n",
        "Technology is my favorite topic because it constantly evolves and shapes our world.\n",
        "I enjoy learning about new gadgets, innovative software, and breakthroughs in artificial intelligence.\n",
        "It's fascinating to see how technology impacts industries like healthcare, education, and entertainment.\n",
        "Reading tech blogs and watching product launches excites me.\n",
        "Technology makes our lives more convenient and opens up endless possibilities for the future.\n",
        "\"\"\"\n",
        "\n",
        "# 1. Custom Tokenization Function\n",
        "def custom_tokenizer(text):\n",
        "    # a. Remove punctuation but keep contractions and hyphenated words\n",
        "    text = re.sub(r'[^\\w\\s\\'-]', '', text)\n",
        "\n",
        "    # b. Tokenize, keeping decimal numbers intact\n",
        "    tokens = []\n",
        "    for word in word_tokenize(text):\n",
        "        if re.match(r'\\d+\\.\\d+', word):  # Check for decimal numbers\n",
        "            tokens.append(word)\n",
        "        else:\n",
        "            tokens.extend(word_tokenize(word))\n",
        "    return tokens\n",
        "\n",
        "# --- Custom Tokenization Output ---\n",
        "custom_tokens = custom_tokenizer(paragraph)\n",
        "print(\"\\nCustom Tokenized Text:\", custom_tokens)\n",
        "\n",
        "# --- 2. Regex Substitutions for Cleaning ---\n",
        "\n",
        "# a. Replace email addresses with '<EMAIL>'\n",
        "text_with_emails = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', '<EMAIL>', paragraph)\n",
        "\n",
        "# b. Replace URLs with '<URL>'\n",
        "text_with_urls = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '<URL>', text_with_emails)\n",
        "\n",
        "# c. Replace phone numbers with '<PHONE>'\n",
        "text_cleaned = re.sub(r'(\\+?\\d{1,2}\\s?)?(\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4})', '<PHONE>', text_with_urls)\n",
        "\n",
        "# --- Cleaning Output ---\n",
        "print(\"\\nText with Emails, URLs, and Phone Numbers Replaced:\")\n",
        "print(text_cleaned)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}